[{"role": "system", "content": "You are a query\u2010generation assistant. For each document you receive, Your task is to generate exactly 5 distinct questions based on the provided document, topics, and keywords.\nYour output MUST adhere to the following format STRICTLY:\n\nRules:\n- Exactly 5 questions.\n- Each question on a new line.\n- NO introductory or concluding text (e.g., \"Here are the questions:\", \"Okay, here are...\", \"These are the queries:\").\n- NO numbering (e.g., \"1.\", \"2.\").\n- NO bullet points or other list markers (e.g., \"-\", \"*\").\n- Your entire response must consist ONLY of the 5 questions, each separated by a newline character.**\n\nWhen generating the queries, document content, topics and keywords information should all be considered:\n\n- **Implicitly cover** the document\u2019s topics:\n   - You may express topics via synonyms, broader phrases, or context (e.g., topic \u201csurvival analysis\u201d \u2192 \u201crisk estimation over time\u201d).\n\n- **Explicitly include** the document\u2019s keywords\n   - Embed them naturally (e.g., \u201cbreast cancer death\u201d can appear as is).\n"}, {"role": "user", "content": "Here is an example document with keywords and weighted topics. Generate 5 relevant queries for this document where high-weight topics should steer more of your queries\u2019 framing.\n\nDocument:\nPabble: Parameterised Scribble for Parallel Programming\nMany parallel and distributed message-passing programs are written in a parametric way over available resources, in particular the number of nodes and their topologies, so that a single parallel program can scale over different environments. This paper presents a parameterised protocol description language, Pabble, which can guarantee safety and progress in a large class of practical, complex parameterised message-passing programs through static checking. Pabble can describe an overall interaction topology, using a concise and expressive notation, designed for a variable number of participants arranged in multiple dimensions. These parameterised protocols in turn automatically generate local protocols for type checking parameterised MPI programs for communication safety and deadlock freedom. In spite of undecidability of endpoint projection and type checking in the underlying parameterised session type theory, our method guarantees the termination of endpoint projection and type checking.\n\nKeywords:\nprogramming language, programmers, formal language, java, implementations, object oriented, computer, interactive, introduces, generic\n\nTopics (with weights):\nProgramming Language Design and Implementation (1.0)\n"}]
[{"role": "system", "content": "You are a query\u2010generation assistant. For each document you receive, Your task is to generate exactly 5 distinct questions based on the provided document, topics, and keywords.\nYour output MUST adhere to the following format STRICTLY:\n\nRules:\n- Exactly 5 questions.\n- Each question on a new line.\n- NO introductory or concluding text (e.g., \"Here are the questions:\", \"Okay, here are...\", \"These are the queries:\").\n- NO numbering (e.g., \"1.\", \"2.\").\n- NO bullet points or other list markers (e.g., \"-\", \"*\").\n- Your entire response must consist ONLY of the 5 questions, each separated by a newline character.**\n\nWhen generating the queries, document content, topics and keywords information should all be considered:\n\n- **Implicitly cover** the document\u2019s topics:\n   - You may express topics via synonyms, broader phrases, or context (e.g., topic \u201csurvival analysis\u201d \u2192 \u201crisk estimation over time\u201d).\n\n- **Explicitly include** the document\u2019s keywords\n   - Embed them naturally (e.g., \u201cbreast cancer death\u201d can appear as is).\n"}, {"role": "user", "content": "Here is an example document with keywords and weighted topics. Generate 5 relevant queries for this document where high-weight topics should steer more of your queries\u2019 framing.\n\nDocument:\nCeramic style change and neutral evolution: a case study from Neolithic Europe\nFollowing on the work of Dunnell, the evolutionary archaeology school has made a sharp distinction between functional and stylistic variation in archaeological artifacts. Variation is defined as functional if it is affected by selection processes and as stylistic if it is a result of processes of random drift. The argument has been further developed by Neiman (1995), who showed by simulation that processes of cultural mutation and drift could produce the kinds of battleship curves that generally characterize artifact-style frequency distributions through time, and also demonstrated that they could account for patterns of stylistic variation through time in Woodland-period ceramic assemblages from Illinois. In this paper we present a case study of change in the decoration of pottery from early Neolithic Central Europe. We show that the actual diachronic frequency distributions and those expected under the neutral model do not coincide and conclude that in this case the neutral model does not provide an adequate description of change in ceramic decoration. A model involving selection, in the form of a bias in favor of novelty in the later phases of the period studied, seems likely to be more appropriate, and we note the social interpretation of the original investigator of the data. In conclusion, it is suggested that neutral models provide an important heuristic tool but that there is not a radical break between functional and stylistic variation.\n\nKeywords:\npainting, visual features, similarity measures, semantic, modern, learning approaches, reputation, case study, market, historical\n\nTopics (with weights):\nArtistic Style and Aesthetic Evaluation of Abstract Paintings (1.0)\n"}]
[{"role": "system", "content": "You are a query\u2010generation assistant. For each document you receive, Your task is to generate exactly 5 distinct questions based on the provided document, topics, and keywords.\nYour output MUST adhere to the following format STRICTLY:\n\nRules:\n- Exactly 5 questions.\n- Each question on a new line.\n- NO introductory or concluding text (e.g., \"Here are the questions:\", \"Okay, here are...\", \"These are the queries:\").\n- NO numbering (e.g., \"1.\", \"2.\").\n- NO bullet points or other list markers (e.g., \"-\", \"*\").\n- Your entire response must consist ONLY of the 5 questions, each separated by a newline character.**\n\nWhen generating the queries, document content, topics and keywords information should all be considered:\n\n- **Implicitly cover** the document\u2019s topics:\n   - You may express topics via synonyms, broader phrases, or context (e.g., topic \u201csurvival analysis\u201d \u2192 \u201crisk estimation over time\u201d).\n\n- **Explicitly include** the document\u2019s keywords\n   - Embed them naturally (e.g., \u201cbreast cancer death\u201d can appear as is).\n"}, {"role": "user", "content": "Here is an example document with keywords and weighted topics. Generate 5 relevant queries for this document where high-weight topics should steer more of your queries\u2019 framing.\n\nDocument:\nOrc-based Learning - Evaluating a Game-Based Learning Approach\nUsing game mechanics to improve the motivation and efforts becomes a popular approach. Especially in higher education many projects have been realized to create a greater engagement of students in learning processes. Because of these innovative ideas there is a lack of corresponding evaluation methods which respect all relevant aspects. For this we created an evaluation model to meet the needs for evaluating a game-based learning approach.\n\nKeywords:\ngame, team, reinforcement learning, equilibrium, rewards, board, agents, learning tools, learning analytics, classroom, databases, previous studies, big data\n\nTopics (with weights):\nGame Theory and Reinforcement Learning (0.5), Adaptive Learning and Assessment Tools for Student Engagement (0.25), Limitations and Utilization of Databases in Research (0.25)\n"}]
[{"role": "system", "content": "You are a query\u2010generation assistant. For each document you receive, Your task is to generate exactly 5 distinct questions based on the provided document, topics, and keywords.\nYour output MUST adhere to the following format STRICTLY:\n\nRules:\n- Exactly 5 questions.\n- Each question on a new line.\n- NO introductory or concluding text (e.g., \"Here are the questions:\", \"Okay, here are...\", \"These are the queries:\").\n- NO numbering (e.g., \"1.\", \"2.\").\n- NO bullet points or other list markers (e.g., \"-\", \"*\").\n- Your entire response must consist ONLY of the 5 questions, each separated by a newline character.**\n\nWhen generating the queries, document content, topics and keywords information should all be considered:\n\n- **Implicitly cover** the document\u2019s topics:\n   - You may express topics via synonyms, broader phrases, or context (e.g., topic \u201csurvival analysis\u201d \u2192 \u201crisk estimation over time\u201d).\n\n- **Explicitly include** the document\u2019s keywords\n   - Embed them naturally (e.g., \u201cbreast cancer death\u201d can appear as is).\n"}, {"role": "user", "content": "Here is an example document with keywords and weighted topics. Generate 5 relevant queries for this document where high-weight topics should steer more of your queries\u2019 framing.\n\nDocument:\nA pairwise ranking based approach to learning with positive and unlabeled examples\nA large fraction of binary classification problems arising in web applications are of the type where the positive class is well defined and compact while the negative class comprises everything else in the distribution for which the classifier is developed; it is hard to represent and sample from such a broad negative class. Classifiers based only on positive and unlabeled examples reduce human annotation effort significantly by removing the burden of choosing a representative set of negative examples. Various methods have been proposed in the literature for building such classifiers. Of these, the state of the art methods are Biased SVM and Elkan & Noto's methods. While these methods often work well in practice, they are computationally expensive since hyperparameter tuning is very important, particularly when the size of labeled positive examples set is small and class imbalance is high. In this paper we propose a pairwise ranking based approach to learn from positive and unlabeled examples (LPU) and we give a theoretical justification for it. We present a pairwise RankSVM (RSVM) based method for our approach. The method is simple, efficient, and its hyperparameters are easy to tune. A detailed experimental study using several benchmark datasets shows that the proposed method gives competitive classification performance compared to the mentioned state of the art methods, while training 3-10 times faster. We also propose an efficient AUC based feature selection technique in the LPU setting and demonstrate its usefulness on the datasets. To get an idea of the goodness of the LPU methods we compare them against supervised learning (SL) methods that also make use of negative examples in training. SL methods give a slightly better performance than LPU methods when there is a rich set of negative examples; however, they are inferior when the number of negative training examples is not large enough.\n\nKeywords:\nclassification problems, class imbalance, class problems, supervised classification, different classification, multi class, computational complexity, data expensive, models large, significantly reduce, efficient, results illustrate\n\nTopics (with weights):\nMulti-label classification and class imbalance issues (0.25), Multi-class classification using ensemble of one-class classifiers (0.25), Computational Complexity in Machine Learning Models (0.25), Efficient Estimation and Computational Complexity Reduction (0.25)\n"}]
[{"role": "system", "content": "You are a query\u2010generation assistant. For each document you receive, Your task is to generate exactly 5 distinct questions based on the provided document, topics, and keywords.\nYour output MUST adhere to the following format STRICTLY:\n\nRules:\n- Exactly 5 questions.\n- Each question on a new line.\n- NO introductory or concluding text (e.g., \"Here are the questions:\", \"Okay, here are...\", \"These are the queries:\").\n- NO numbering (e.g., \"1.\", \"2.\").\n- NO bullet points or other list markers (e.g., \"-\", \"*\").\n- Your entire response must consist ONLY of the 5 questions, each separated by a newline character.**\n\nWhen generating the queries, document content, topics and keywords information should all be considered:\n\n- **Implicitly cover** the document\u2019s topics:\n   - You may express topics via synonyms, broader phrases, or context (e.g., topic \u201csurvival analysis\u201d \u2192 \u201crisk estimation over time\u201d).\n\n- **Explicitly include** the document\u2019s keywords\n   - Embed them naturally (e.g., \u201cbreast cancer death\u201d can appear as is).\n"}, {"role": "user", "content": "Here is an example document with keywords and weighted topics. Generate 5 relevant queries for this document where high-weight topics should steer more of your queries\u2019 framing.\n\nDocument:\nSyntactic Annotation of a German Newspaper Corpus\nWe report on the syntactic annotation of a German newspaper corpus. The annotations consist of context-free structures, additionally allowing crossing branches, with labeled nodes (phrases) and edges (grammatical functions). Furthermore, we present a new, interactive semi-automatic annotation process that allows efficient and reliable annotations. The annotation process is sped up by incrementally presenting structures and by automatically highlighting unreliable assignments.\n\nKeywords:\ngerman, english based, word features, semantically related, model experiments, text generation, discourse relations, translation task, results demonstrated, comprehensive evaluation\n\nTopics (with weights):\nGerman-English Translation and Textual Similarity Detection (1.0)\n"}]
[{"role": "system", "content": "You are a query\u2010generation assistant. For each document you receive, Your task is to generate exactly 5 distinct questions based on the provided document, topics, and keywords.\nYour output MUST adhere to the following format STRICTLY:\n\nRules:\n- Exactly 5 questions.\n- Each question on a new line.\n- NO introductory or concluding text (e.g., \"Here are the questions:\", \"Okay, here are...\", \"These are the queries:\").\n- NO numbering (e.g., \"1.\", \"2.\").\n- NO bullet points or other list markers (e.g., \"-\", \"*\").\n- Your entire response must consist ONLY of the 5 questions, each separated by a newline character.**\n\nWhen generating the queries, document content, topics and keywords information should all be considered:\n\n- **Implicitly cover** the document\u2019s topics:\n   - You may express topics via synonyms, broader phrases, or context (e.g., topic \u201csurvival analysis\u201d \u2192 \u201crisk estimation over time\u201d).\n\n- **Explicitly include** the document\u2019s keywords\n   - Embed them naturally (e.g., \u201cbreast cancer death\u201d can appear as is).\n"}, {"role": "user", "content": "Here is an example document with keywords and weighted topics. Generate 5 relevant queries for this document where high-weight topics should steer more of your queries\u2019 framing.\n\nDocument:\nA Data-Oriented Approach to Semantic Interpretation\nIn Data-Oriented Parsing (DOP), an annotated language corpus is used as a stochastic grammar. The most probable analysis of a new input sentence is constructed by combining sub-analyses from the corpus in the most probable way. This approach has been succesfully used for syntactic analysis, using corpora with syntactic annotations such as the Penn Treebank. If a corpus with semantically annotated sentences is used, the same approach can also generate the most probable semantic interpretation of an input sentence. The present paper explains this semantic interpretation method, and summarizes the results of a preliminary experiment. Semantic annotations were added to the syntactic annotations of most of the sentences of the ATIS corpus. A data-oriented semantic interpretation algorithm was succesfully tested on this semantically enriched corpus.\n\nKeywords:\nannotated corpus, input sentence, semantic interpretation, analyses, natural language, grammatical, theoretical analysis, grammars, language processing, corpus, english translation, results experiments, language data\n\nTopics (with weights):\nSemantic Interpretation Generation from Annotated Corpora (0.5), Grammar Inference and Stochastic Context-Free Parsing (0.25), Machine Translation and Document Summarization for Low-Resource Languages (0.25)\n"}]
[{"role": "system", "content": "You are a query\u2010generation assistant. For each document you receive, Your task is to generate exactly 5 distinct questions based on the provided document, topics, and keywords.\nYour output MUST adhere to the following format STRICTLY:\n\nRules:\n- Exactly 5 questions.\n- Each question on a new line.\n- NO introductory or concluding text (e.g., \"Here are the questions:\", \"Okay, here are...\", \"These are the queries:\").\n- NO numbering (e.g., \"1.\", \"2.\").\n- NO bullet points or other list markers (e.g., \"-\", \"*\").\n- Your entire response must consist ONLY of the 5 questions, each separated by a newline character.**\n\nWhen generating the queries, document content, topics and keywords information should all be considered:\n\n- **Implicitly cover** the document\u2019s topics:\n   - You may express topics via synonyms, broader phrases, or context (e.g., topic \u201csurvival analysis\u201d \u2192 \u201crisk estimation over time\u201d).\n\n- **Explicitly include** the document\u2019s keywords\n   - Embed them naturally (e.g., \u201cbreast cancer death\u201d can appear as is).\n"}, {"role": "user", "content": "Here is an example document with keywords and weighted topics. Generate 5 relevant queries for this document where high-weight topics should steer more of your queries\u2019 framing.\n\nDocument:\nObjects, Associations and Subsystems: A Hierarchical Approach to Encapsulation\nWe describe a compositional approach to the formal interpretation of type view diagrams and statecharts. We define theories for object instances and classes, and theories for associations between them. These theories are combined with categorical constructions to yield a formalisation of the entire system. We observe that some notations require the identification of theories intermediate between the theories of the constituent classes and associations and that of the entire system. This leads us to propose a notion of subsystem which generalises the concept of object and yields an approach to system specification employing object-like encapsulation in a nested hierarchy of components.\n\nKeywords:\nprogramming languages, model language, semantics, linear time, present framework, formal, goals, ontology, domain knowledge, semantic web, textual entailment, semantic types, information extraction, web applications\n\nTopics (with weights):\nTemporal Logic and Formal Semantics in Programming Languages (0.5), Visual Entailment and Textual Recognition using Semantic Web Ontology (0.5)\n"}]
[{"role": "system", "content": "You are a query\u2010generation assistant. For each document you receive, Your task is to generate exactly 5 distinct questions based on the provided document, topics, and keywords.\nYour output MUST adhere to the following format STRICTLY:\n\nRules:\n- Exactly 5 questions.\n- Each question on a new line.\n- NO introductory or concluding text (e.g., \"Here are the questions:\", \"Okay, here are...\", \"These are the queries:\").\n- NO numbering (e.g., \"1.\", \"2.\").\n- NO bullet points or other list markers (e.g., \"-\", \"*\").\n- Your entire response must consist ONLY of the 5 questions, each separated by a newline character.**\n\nWhen generating the queries, document content, topics and keywords information should all be considered:\n\n- **Implicitly cover** the document\u2019s topics:\n   - You may express topics via synonyms, broader phrases, or context (e.g., topic \u201csurvival analysis\u201d \u2192 \u201crisk estimation over time\u201d).\n\n- **Explicitly include** the document\u2019s keywords\n   - Embed them naturally (e.g., \u201cbreast cancer death\u201d can appear as is).\n"}, {"role": "user", "content": "Here is an example document with keywords and weighted topics. Generate 5 relevant queries for this document where high-weight topics should steer more of your queries\u2019 framing.\n\nDocument:\nArabic Keyphrase Extraction using Linguistic knowledge and Machine Learning Techniques\nIn this paper, a supervised learning technique for extracting keyphrases of Arabic documents is presented. The extractor is supplied with linguistic knowledge to enhance its efficiency instead of relying only on statistical information such as term frequency and distance. During analysis, an annotated Arabic corpus is used to extract the required lexical features of the document words. The knowledge also includes syntactic rules based on part of speech tags and allowed word sequences to extract the candidate keyphrases. In this work, the abstract form of Arabic words is used instead of its stem form to represent the candidate terms. The Abstract form hides most of the inflections found in Arabic words. The paper introduces new features of keyphrases based on linguistic knowledge, to capture titles and subtitles of a document. A simple ANOVA test is used to evaluate the validity of selected features. Then, the learning model is built using the LDA - Linear Discriminant Analysis - and training documents. Although, the presented system is trained using documents in the IT domain, experiments carried out show that it has a significantly better performance than the existing Arabic extractor systems, where precision and recall values reach double their corresponding values in the other systems especially for lengthy and non-scientific articles.\n\nKeywords:\narabic, words used, ontology, orthographic, information retrieval, existing techniques, corpus statistics, sentiment analysis, analyzer, knowledge bases, variety applications, feature set, classification accuracy\n\nTopics (with weights):\nArabic Language and Ontology Development (0.666667), Text Knowledge Representation and Application (0.166667), Feature selection and preprocessing for classification accuracy improvement (0.166667)\n"}]
[{"role": "system", "content": "You are a query\u2010generation assistant. For each document you receive, Your task is to generate exactly 5 distinct questions based on the provided document, topics, and keywords.\nYour output MUST adhere to the following format STRICTLY:\n\nRules:\n- Exactly 5 questions.\n- Each question on a new line.\n- NO introductory or concluding text (e.g., \"Here are the questions:\", \"Okay, here are...\", \"These are the queries:\").\n- NO numbering (e.g., \"1.\", \"2.\").\n- NO bullet points or other list markers (e.g., \"-\", \"*\").\n- Your entire response must consist ONLY of the 5 questions, each separated by a newline character.**\n\nWhen generating the queries, document content, topics and keywords information should all be considered:\n\n- **Implicitly cover** the document\u2019s topics:\n   - You may express topics via synonyms, broader phrases, or context (e.g., topic \u201csurvival analysis\u201d \u2192 \u201crisk estimation over time\u201d).\n\n- **Explicitly include** the document\u2019s keywords\n   - Embed them naturally (e.g., \u201cbreast cancer death\u201d can appear as is).\n"}, {"role": "user", "content": "Here is an example document with keywords and weighted topics. Generate 5 relevant queries for this document where high-weight topics should steer more of your queries\u2019 framing.\n\nDocument:\nDomain specialization: a post-training domain adaptation for Neural Machine Translation\nDomain adaptation is a key feature in Machine Translation. It generally encompasses terminology, domain and style adaptation, especially for human post-editing workflows in Computer Assisted Translation (CAT). With Neural Machine Translation (NMT), we introduce a new notion of domain adaptation that we call\"specialization\"and which is showing promising results both in the learning speed and in adaptation accuracy. In this paper, we propose to explore this approach under several perspectives.\n\nKeywords:\ndomain adaptation, translation models, machine translation, domain data, translation smt, training data, translation nmt, neural machine, future research, directions future, potential applications, possible solutions, research area\n\nTopics (with weights):\nDomain adaptation in statistical machine translation (0.333333), Neural Machine Translation Domain Adaptation Techniques (0.333333), Future Research Directions in a Research Area (0.333333)\n"}]
[{"role": "system", "content": "You are a query\u2010generation assistant. For each document you receive, Your task is to generate exactly 5 distinct questions based on the provided document, topics, and keywords.\nYour output MUST adhere to the following format STRICTLY:\n\nRules:\n- Exactly 5 questions.\n- Each question on a new line.\n- NO introductory or concluding text (e.g., \"Here are the questions:\", \"Okay, here are...\", \"These are the queries:\").\n- NO numbering (e.g., \"1.\", \"2.\").\n- NO bullet points or other list markers (e.g., \"-\", \"*\").\n- Your entire response must consist ONLY of the 5 questions, each separated by a newline character.**\n\nWhen generating the queries, document content, topics and keywords information should all be considered:\n\n- **Implicitly cover** the document\u2019s topics:\n   - You may express topics via synonyms, broader phrases, or context (e.g., topic \u201csurvival analysis\u201d \u2192 \u201crisk estimation over time\u201d).\n\n- **Explicitly include** the document\u2019s keywords\n   - Embed them naturally (e.g., \u201cbreast cancer death\u201d can appear as is).\n"}, {"role": "user", "content": "Here is an example document with keywords and weighted topics. Generate 5 relevant queries for this document where high-weight topics should steer more of your queries\u2019 framing.\n\nDocument:\nInteractive verification of game design and playing strategies\nReinforcement learning is considered as one of the most suitable and prominent methods for solving game problems due to its capability to discover good strategies by extended se self-training and limited initial knowledge. In this paper we elaborate on using reinforcement learning for verifying game designs and playing strategies. Specifically, we examine a new strategy game that has been trained on self-playing games and analyze the game performance after human interaction. We demonstrate, through selected game instances, the impact of human interference to the learning process, and eventually the game design.\n\nKeywords:\ngame, team, reinforcement learning, equilibrium, rewards, board, agents, intelligence, novice, action\n\nTopics (with weights):\nGame Theory and Reinforcement Learning (1.0)\n"}]
