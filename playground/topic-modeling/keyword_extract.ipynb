{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b8da175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3633\n",
      "{'_id': 'MED-10', 'title': 'Statin Use and Breast Cancer Survival: A Nationwide Cohort Study from Finland', 'text': 'Recent studies have suggested that statins, an established drug group in the prevention of cardiovascular mortality, could delay or prevent breast cancer recurrence but the effect on disease-specific mortality remains unclear. We evaluated risk of breast cancer death among statin users in a population-based cohort of breast cancer patients. The study cohort included all newly diagnosed breast cancer patients in Finland during 1995–2003 (31,236 cases), identified from the Finnish Cancer Registry. Information on statin use before and after the diagnosis was obtained from a national prescription database. We used the Cox proportional hazards regression method to estimate mortality among statin users with statin use as time-dependent variable. A total of 4,151 participants had used statins. During the median follow-up of 3.25 years after the diagnosis (range 0.08–9.0 years) 6,011 participants died, of which 3,619 (60.2%) was due to breast cancer. After adjustment for age, tumor characteristics, and treatment selection, both post-diagnostic and pre-diagnostic statin use were associated with lowered risk of breast cancer death (HR 0.46, 95% CI 0.38–0.55 and HR 0.54, 95% CI 0.44–0.67, respectively). The risk decrease by post-diagnostic statin use was likely affected by healthy adherer bias; that is, the greater likelihood of dying cancer patients to discontinue statin use as the association was not clearly dose-dependent and observed already at low-dose/short-term use. The dose- and time-dependence of the survival benefit among pre-diagnostic statin users suggests a possible causal effect that should be evaluated further in a clinical trial testing statins’ effect on survival in breast cancer patients.', 'metadata': {'url': 'http://www.ncbi.nlm.nih.gov/pubmed/25329299'}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "CORPUS_PATH = \"/home/guest/r12922050/GitHub/d2qplus/data/nfcorpus/corpus.jsonl\"\n",
    "with open(CORPUS_PATH, \"r\") as f:\n",
    "    corpus = [json.loads(line) for line in f]\n",
    "print(len(corpus))\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d8d29f",
   "metadata": {},
   "source": [
    "# Keyword Extraction using KeyBERT + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9496608e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name allenai/scibert_scivocab_uncased. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of results generated:3633\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1,3),      # or (1,1)/(2,2) depending on your n-gram needs\n",
    "    max_df=0.9,             # drop very frequent tokens\n",
    "    min_df=2,               # drop extremely rare tokens\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "\n",
    "tfidf.fit([doc[\"title\"] + \" \" + doc[\"text\"] for doc in corpus])\n",
    "\n",
    "# mp_net = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\", device=\"cuda:2\")\n",
    "sci_bert = SentenceTransformer(\"allenai/scibert_scivocab_uncased\", device=\"cuda:2\")\n",
    "\n",
    "\n",
    "kw_model = KeyBERT(model=sci_bert)\n",
    "\n",
    "extract_params = {\n",
    "    \"keyphrase_ngram_range\": (1, 3),  # unigrams + bigrams\n",
    "    \"stop_words\": \"english\",          # default English stop words\n",
    "    \"use_mmr\": True,                  # use Maximal Marginal Relevance to increase diversity\n",
    "    \"diversity\": 0.6,                 # diversity trade-off between relevance vs novelty\n",
    "    \"top_n\": 15,                       # extract up to 10 keyphrases per document\n",
    "    \"vectorizer\": tfidf,             # use the fitted TF-IDF vectorizer\n",
    "}\n",
    "\n",
    "all_texts = [doc[\"title\"] + \" \" + doc[\"text\"] for doc in corpus]\n",
    "results = kw_model.extract_keywords(all_texts, **extract_params)\n",
    "print(f\"# of results generated:{len(results)}\")\n",
    "\n",
    "doc_ids = [doc[\"doc_id\"] for doc in corpus]\n",
    "# zip doc_ids with results\n",
    "\n",
    "\n",
    "\n",
    "# save to jsonl\n",
    "import json\n",
    "OUTPUT_PATH = \"/home/guest/r12922050/GitHub/d2qplus/augmented-data/nfcorpus/keywords/scibert_1_3_gram.jsonl\"\n",
    "with open(OUTPUT_PATH, \"w\") as f:\n",
    "    for doc_id, keywords in keywords_per_doc.items():\n",
    "        f.write(json.dumps({\"doc_id\": doc_id, \"keywords\": keywords}) + \"\\n\")\n",
    "print(f\"saved keywords to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f17e3de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved keywords to /home/guest/r12922050/GitHub/d2qplus/augmented-data/nfcorpus/keywords/scibert_1_3_gram.jsonl\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = \"/home/guest/r12922050/GitHub/d2qplus/augmented-data/nfcorpus/keywords/scibert_1_3_gram.jsonl\"\n",
    "with open(OUTPUT_PATH, \"w\") as f:\n",
    "    for i in range(len(corpus)):\n",
    "        doc_id = corpus[i][\"_id\"]\n",
    "        title = corpus[i][\"title\"]\n",
    "        keywords = results[i]\n",
    "        f.write(json.dumps({\"doc_id\": doc_id, \"title\": title, \"keywords\": keywords}) + \"\\n\")\n",
    "print(f\"saved keywords to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17a0bf8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': 'MED-10',\n",
       " 'title': 'Statin Use and Breast Cancer Survival: A Nationwide Cohort Study from Finland',\n",
       " 'text': 'Recent studies have suggested that statins, an established drug group in the prevention of cardiovascular mortality, could delay or prevent breast cancer recurrence but the effect on disease-specific mortality remains unclear. We evaluated risk of breast cancer death among statin users in a population-based cohort of breast cancer patients. The study cohort included all newly diagnosed breast cancer patients in Finland during 1995–2003 (31,236 cases), identified from the Finnish Cancer Registry. Information on statin use before and after the diagnosis was obtained from a national prescription database. We used the Cox proportional hazards regression method to estimate mortality among statin users with statin use as time-dependent variable. A total of 4,151 participants had used statins. During the median follow-up of 3.25 years after the diagnosis (range 0.08–9.0 years) 6,011 participants died, of which 3,619 (60.2%) was due to breast cancer. After adjustment for age, tumor characteristics, and treatment selection, both post-diagnostic and pre-diagnostic statin use were associated with lowered risk of breast cancer death (HR 0.46, 95% CI 0.38–0.55 and HR 0.54, 95% CI 0.44–0.67, respectively). The risk decrease by post-diagnostic statin use was likely affected by healthy adherer bias; that is, the greater likelihood of dying cancer patients to discontinue statin use as the association was not clearly dose-dependent and observed already at low-dose/short-term use. The dose- and time-dependence of the survival benefit among pre-diagnostic statin users suggests a possible causal effect that should be evaluated further in a clinical trial testing statins’ effect on survival in breast cancer patients.',\n",
       " 'metadata': {'url': 'http://www.ncbi.nlm.nih.gov/pubmed/25329299'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d2d203b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3633 {'MED-10': [['possible causal effect', 0.5809], ['nationwide cohort study', 0.5649], ['discontinue statin use', 0.5625], ['cancer registry information', 0.5324], ['characteristics treatment selection', 0.5257], ['breast cancer', 0.5231], ['specific mortality', 0.518], ['tumor characteristics', 0.517], ['95 ci 44', 0.4994], ['clinical trial testing', 0.4993], ['patients finland', 0.4434], ['users population based', 0.4357], ['2003 31', 0.404], ['ci', 0.4002], ['54', 0.3239]]}\n"
     ]
    }
   ],
   "source": [
    "with open(f\"/home/guest/r12922050/GitHub/d2qplus/augmented-data/nfcorpus/keywords/scibert_1_3_gram.jsonl\", \"r\") as f:\n",
    "    corpus_keywords = [json.loads(line) for line in f]\n",
    "print(len(corpus_keywords), corpus_keywords[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216b2320",
   "metadata": {},
   "source": [
    "## Core keyword extraction\n",
    "\n",
    "> CCQGen\n",
    "\n",
    "Core phrases identification. From each document, we\n",
    "identify core phrases used to describe its concepts. These phrases\n",
    "offer fine-grained details not captured at the topic level. We note\n",
    "that not all phrases in the document are equally important. Core\n",
    "phrases should describe concepts strongly relevant to the document\n",
    "but not frequently covered by other documents with similar topics.\n",
    "For example, among documents about ‘recommender system’ topic,\n",
    "the phrase ‘user-item interaction’ is very commonly used, and less\n",
    "likely to represent the most important concepts of the document.\n",
    "\n",
    "所以我們也應該需要考慮 keywords relevant to this document but not frequently covered by other documents with similar topics. 而不是直接使用整個 topic 的 keyword list 來去 guide generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cffdd45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_TOPICS_PATH = \"/home/guest/r12922050/GitHub/d2qplus/augmented-data/CSFCube-1.1/topics/0609-pritamdeka_scibert-biobert-pos-keybert-mmr/doc_topics.jsonl\"\n",
    "TOPIC_INFO_PKL = \"/home/guest/r12922050/GitHub/d2qplus/augmented-data/CSFCube-1.1/topics/0609-pritamdeka_scibert-biobert-pos-keybert-mmr/topic_info_dataframe_enhanced.pkl\"\n",
    "CORPUS_PATH = \"/home/guest/r12922050/GitHub/d2qplus/data/CSFCube-1.1/corpus.jsonl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5090c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4207 {'doc_id': '16421850', 'topics': [{'topic_id': 89, 'weight': 0.25}, {'topic_id': 36, 'weight': 0.25}, {'topic_id': 72, 'weight': 0.25}, {'topic_id': 23, 'weight': 0.25}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(DOC_TOPICS_PATH, \"r\") as f:\n",
    "    doc_topics = [json.loads(line) for line in f]\n",
    "print(len(doc_topics), doc_topics[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8825a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from keybert import KeyBERT\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Sample documents\n",
    "docs = [doc['text'] for doc in all_docs][:50]\n",
    "\n",
    "# Step 1: Use BERTopic to identify topics\n",
    "topic_model = BERTopic(calculate_probabilities=True, verbose=True)\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "# Step 2: Group documents by topic\n",
    "topic_docs = {}\n",
    "for doc, topic in zip(docs, topics):\n",
    "    if topic not in topic_docs:\n",
    "        topic_docs[topic] = []\n",
    "    topic_docs[topic].append(doc)\n",
    "\n",
    "# Step 3: Initialize KeyBERT\n",
    "kw_model = KeyBERT()\n",
    "\n",
    "# Step 4: Extract topic-level keywords to understand the topic's general content\n",
    "topic_keywords = {}\n",
    "for topic, topic_doc_list in topic_docs.items():\n",
    "    if topic == -1:  # Skip outliers\n",
    "        continue\n",
    "    # Combine all documents in the topic to get topic-level keywords\n",
    "    combined_text = \" \".join(topic_doc_list)\n",
    "    topic_keywords[topic] = kw_model.extract_keywords(\n",
    "        combined_text,\n",
    "        keyphrase_ngram_range=(1, 2),\n",
    "        stop_words='english',\n",
    "        top_n=10\n",
    "    )\n",
    "\n",
    "# Step 5: Extract document-level keywords and filter based on topic context\n",
    "document_keywords = []\n",
    "for doc_idx, (doc, topic) in enumerate(zip(docs, topics)):\n",
    "    if topic == -1:  # Skip outliers\n",
    "        document_keywords.append((doc, []))\n",
    "        continue\n",
    "    \n",
    "    # Extract document-level keywords\n",
    "    doc_keywords = kw_model.extract_keywords(\n",
    "        doc,\n",
    "        keyphrase_ngram_range=(1, 2),\n",
    "        stop_words='english',\n",
    "        top_n=5,\n",
    "        use_mmr=True,\n",
    "        diversity=0.5\n",
    "    )\n",
    "    \n",
    "    # Get topic-level keywords for comparison\n",
    "    topic_kw = topic_keywords.get(topic, [])\n",
    "    topic_kw_set = set(kw[0] for kw in topic_kw)\n",
    "    \n",
    "    # Filter document keywords: keep only those that are not too common in the topic\n",
    "    filtered_keywords = []\n",
    "    for keyword, score in doc_keywords:\n",
    "        # Check if the keyword is in the top topic keywords (i.e., too common in the topic)\n",
    "        if keyword not in topic_kw_set:\n",
    "            filtered_keywords.append((keyword, score))\n",
    "        else:\n",
    "            # Optionally, reduce the score of common keywords instead of filtering them out\n",
    "            adjusted_score = score * 0.5  # Reduce score for common keywords\n",
    "            filtered_keywords.append((keyword, adjusted_score))\n",
    "    \n",
    "    # Sort by adjusted score and take top 5\n",
    "    filtered_keywords = sorted(filtered_keywords, key=lambda x: x[1], reverse=True)[:5]\n",
    "    document_keywords.append((doc, filtered_keywords))\n",
    "\n",
    "# Step 6: Output the results\n",
    "for doc, keywords in document_keywords:\n",
    "    print(f\"Document: {doc}\")\n",
    "    print(f\"Keywords: {keywords}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a08b4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/r12922050/miniconda3/envs/d2qplus/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 4207 documents with 4207 topic assignments\n",
      "Grouping documents by topics...\n",
      "Creating BM25 models for each topic...\n",
      "Grouping documents by topics...\n",
      "Creating BM25 models for each topic...\n",
      "Extracting core phrases...\n",
      "Extracting core phrases...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 4207/4207 [08:46<00:00,  7.99it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/r12922050/miniconda3/envs/d2qplus/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 4207 documents with 4207 topic assignments\n",
      "Grouping documents by topics...\n",
      "Creating BM25 models for each topic...\n",
      "Grouping documents by topics...\n",
      "Creating BM25 models for each topic...\n",
      "Extracting core phrases...\n",
      "Extracting core phrases...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 4207/4207 [08:46<00:00,  7.99it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core phrases saved to /home/guest/r12922050/GitHub/d2qplus/augmented-data/CSFCube-1.1/keywords/core_phrases_ccqgen.jsonl\n",
      "\n",
      "Example results:\n",
      "\n",
      "Document 7632414:\n",
      "  - 'endpoint projection type' (score: 47828.8412)\n",
      "  - 'parameterised mpi programs' (score: 44512.6552)\n",
      "  - 'dimensions parameterised protocols' (score: 26746.3935)\n",
      "  - 'pabble guarantee safety' (score: 18875.8686)\n",
      "  - 'protocols type checking' (score: 9923.5282)\n",
      "  - 'local protocols type' (score: 7771.4462)\n",
      "\n",
      "Document 143814895:\n",
      "  - 'stylistic variation archaeological' (score: 8.4367)\n",
      "  - 'distinction functional stylistic' (score: 8.4367)\n",
      "  - 'variation defined functional' (score: 7.3520)\n",
      "  - 'conclude case neutral' (score: 5.9623)\n",
      "  - 'simulation processes cultural' (score: 4.1694)\n",
      "  - 'suggested neutral models' (score: 4.1694)\n",
      "\n",
      "Document 62097085:\n",
      "  - 'education projects realized' (score: 26862.4643)\n",
      "  - 'higher education projects' (score: 24264.0732)\n",
      "  - 'innovative ideas lack' (score: 4767.3662)\n",
      "  - 'lack corresponding evaluation' (score: 2584.4696)\n",
      "  - 'realized create greater' (score: 2469.0400)\n",
      "  - 'create greater engagement' (score: 1292.6110)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from keybert import KeyBERT\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CorePhraseExtractor:\n",
    "    \"\"\"\n",
    "    Extract core phrases using the CCQGen methodology with existing topic assignments.\n",
    "    Core phrases are relevant to the document but distinctive within its topic group.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model=\"allenai/scibert_scivocab_uncased\", device=\"cuda:2\"):\n",
    "        \"\"\"Initialize the extractor with KeyBERT and embedding model.\"\"\"\n",
    "        embedder = SentenceTransformer(embedding_model, device=device)\n",
    "        self.kw_model = KeyBERT(model=embedder)\n",
    "        \n",
    "    def extract_core_phrases(self, corpus, doc_topics, \n",
    "                           top_n_candidates=20, \n",
    "                           selection_ratio=0.2,\n",
    "                           min_phrases_per_doc=1,\n",
    "                           max_phrases_per_doc=8,\n",
    "                           keyphrase_ngram_range=(1, 3),\n",
    "                           use_mmr=True,\n",
    "                           diversity=0.7):\n",
    "        \"\"\"\n",
    "        Extract core phrases for each document using the CCQGen distinctiveness score.\n",
    "        \n",
    "        Args:\n",
    "            corpus: List of documents with '_id' and 'text' keys\n",
    "            doc_topics: List of topic assignments with 'doc_id' and 'topics' keys\n",
    "            top_n_candidates: Number of candidate phrases to extract initially\n",
    "            selection_ratio: Ratio of candidates to select as core phrases (0.2 = top 20%)\n",
    "            min_phrases_per_doc: Minimum number of phrases per document\n",
    "            max_phrases_per_doc: Maximum number of phrases per document\n",
    "            keyphrase_ngram_range: N-gram range for phrase extraction\n",
    "            use_mmr: Use Maximal Marginal Relevance for diversity\n",
    "            diversity: Diversity parameter for MMR (higher = more diverse)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create document lookup\n",
    "        doc_lookup = {doc['_id']: doc for doc in corpus}\n",
    "        topic_assignment = {item['doc_id']: item['topics'] for item in doc_topics}\n",
    "        \n",
    "        # Group documents by their assigned topics (weighted by topic strength)\n",
    "        print(\"Grouping documents by topics...\")\n",
    "        topic_to_docs = self._group_docs_by_topics(topic_assignment)\n",
    "        \n",
    "        # Create BM25 models for each topic\n",
    "        print(\"Creating BM25 models for each topic...\")\n",
    "        topic_bm25_models = self._create_topic_bm25_models(topic_to_docs, doc_lookup)\n",
    "        \n",
    "        # Extract core phrases for each document\n",
    "        print(\"Extracting core phrases...\")\n",
    "        doc_core_phrases = {}\n",
    "        \n",
    "        for doc_item in tqdm(doc_topics, desc=\"Processing documents\"):\n",
    "            doc_id = doc_item['doc_id']\n",
    "            doc_topics_list = doc_item['topics']\n",
    "            \n",
    "            if doc_id not in doc_lookup:\n",
    "                continue\n",
    "                \n",
    "            doc_text = doc_lookup[doc_id]['text']\n",
    "            \n",
    "            # Extract candidate phrases with improved KeyBERT parameters\n",
    "            candidate_phrases = self.kw_model.extract_keywords(\n",
    "                doc_text,\n",
    "                keyphrase_ngram_range=keyphrase_ngram_range,\n",
    "                stop_words='english',\n",
    "                top_n=top_n_candidates,\n",
    "                use_mmr=use_mmr,\n",
    "                diversity=diversity,  # Higher diversity to avoid redundant phrases\n",
    "                use_maxsum=False,     # MMR is generally better than MaxSum\n",
    "                nr_candidates=top_n_candidates * 2  # More candidates for MMR to choose from\n",
    "            )\n",
    "            \n",
    "            if not candidate_phrases:\n",
    "                doc_core_phrases[doc_id] = []\n",
    "                continue\n",
    "            \n",
    "            # Calculate distinctiveness scores for each candidate\n",
    "            distinctive_phrases = self._calculate_distinctiveness_scores(\n",
    "                candidate_phrases, doc_id, doc_topics_list, \n",
    "                topic_to_docs, topic_bm25_models, doc_lookup\n",
    "            )\n",
    "            \n",
    "            # Select top phrases based on distinctiveness\n",
    "            num_to_select = max(\n",
    "                min_phrases_per_doc,\n",
    "                min(max_phrases_per_doc, int(len(distinctive_phrases) * selection_ratio))\n",
    "            )\n",
    "            \n",
    "            distinctive_phrases.sort(key=lambda x: x[1], reverse=True)\n",
    "            selected_phrases = distinctive_phrases[:num_to_select]\n",
    "            \n",
    "            doc_core_phrases[doc_id] = [\n",
    "                {\"phrase\": phrase, \"distinctiveness_score\": score} \n",
    "                for phrase, score in selected_phrases\n",
    "            ]\n",
    "        \n",
    "        return doc_core_phrases\n",
    "    \n",
    "    def _group_docs_by_topics(self, topic_assignment):\n",
    "        \"\"\"Group documents by their assigned topics, considering topic weights.\"\"\"\n",
    "        topic_to_docs = defaultdict(list)\n",
    "        \n",
    "        for doc_id, topics in topic_assignment.items():\n",
    "            for topic_info in topics:\n",
    "                topic_id = topic_info['topic_id']\n",
    "                weight = topic_info['weight']\n",
    "                topic_to_docs[topic_id].append({\n",
    "                    'doc_id': doc_id,\n",
    "                    'weight': weight\n",
    "                })\n",
    "        \n",
    "        return topic_to_docs\n",
    "    \n",
    "    def _create_topic_bm25_models(self, topic_to_docs, doc_lookup):\n",
    "        \"\"\"Create BM25 models for each topic.\"\"\"\n",
    "        topic_bm25_models = {}\n",
    "        \n",
    "        for topic_id, doc_list in topic_to_docs.items():\n",
    "            # Get document texts for this topic\n",
    "            docs_in_topic = []\n",
    "            for doc_info in doc_list:\n",
    "                doc_id = doc_info['doc_id']\n",
    "                if doc_id in doc_lookup:\n",
    "                    docs_in_topic.append(doc_lookup[doc_id]['text'])\n",
    "            \n",
    "            if len(docs_in_topic) > 1:  # Need at least 2 docs for BM25 comparison\n",
    "                # Tokenize documents for BM25\n",
    "                tokenized_corpus = [doc.lower().split() for doc in docs_in_topic]\n",
    "                topic_bm25_models[topic_id] = {\n",
    "                    'model': BM25Okapi(tokenized_corpus),\n",
    "                    'doc_ids': [doc_info['doc_id'] for doc_info in doc_list if doc_info['doc_id'] in doc_lookup]\n",
    "                }\n",
    "        \n",
    "        return topic_bm25_models\n",
    "    \n",
    "    def _calculate_distinctiveness_scores(self, candidate_phrases, current_doc_id, \n",
    "                                        doc_topics_list, topic_to_docs, \n",
    "                                        topic_bm25_models, doc_lookup):\n",
    "        \"\"\"Calculate distinctiveness scores using CCQGen methodology.\"\"\"\n",
    "        distinctive_phrases = []\n",
    "        \n",
    "        for phrase, relevance_score in candidate_phrases:\n",
    "            # Calculate distinctiveness across all topics this document belongs to\n",
    "            total_distinctiveness = 0.0\n",
    "            total_weight = 0.0\n",
    "            \n",
    "            for topic_info in doc_topics_list:\n",
    "                topic_id = topic_info['topic_id']\n",
    "                topic_weight = topic_info['weight']\n",
    "                \n",
    "                if topic_id not in topic_bm25_models:\n",
    "                    # If topic has insufficient docs for BM25, use relevance score only\n",
    "                    distinctiveness = relevance_score\n",
    "                else:\n",
    "                    bm25_info = topic_bm25_models[topic_id]\n",
    "                    bm25_model = bm25_info['model']\n",
    "                    topic_doc_ids = bm25_info['doc_ids']\n",
    "                    \n",
    "                    # Find current document's position in the topic\n",
    "                    try:\n",
    "                        current_doc_idx = topic_doc_ids.index(current_doc_id)\n",
    "                    except ValueError:\n",
    "                        # Document not found in topic (shouldn't happen)\n",
    "                        distinctiveness = relevance_score\n",
    "                        continue\n",
    "                    \n",
    "                    # Calculate BM25 scores for the phrase across all docs in topic\n",
    "                    tokenized_phrase = phrase.lower().split()\n",
    "                    all_scores = bm25_model.get_scores(tokenized_phrase)\n",
    "                    \n",
    "                    # Current document's BM25 score\n",
    "                    current_score = all_scores[current_doc_idx]\n",
    "                    \n",
    "                    # Sum of exp(BM25) for other documents in topic\n",
    "                    other_scores = np.concatenate([\n",
    "                        all_scores[:current_doc_idx], \n",
    "                        all_scores[current_doc_idx+1:]\n",
    "                    ])\n",
    "                    sum_exp_others = np.sum(np.exp(other_scores))\n",
    "                    \n",
    "                    # CCQGen distinctiveness formula\n",
    "                    distinctiveness = np.exp(current_score) / (1 + sum_exp_others)\n",
    "                \n",
    "                total_distinctiveness += distinctiveness * topic_weight\n",
    "                total_weight += topic_weight\n",
    "            \n",
    "            # Weight-averaged distinctiveness score\n",
    "            final_distinctiveness = total_distinctiveness / total_weight if total_weight > 0 else relevance_score\n",
    "            distinctive_phrases.append((phrase, final_distinctiveness))\n",
    "        \n",
    "        return distinctive_phrases\n",
    "\n",
    "# Usage example with your data\n",
    "def run_core_phrase_extraction():\n",
    "    \"\"\"Run core phrase extraction on your corpus and doc_topics data.\"\"\"\n",
    "    CORPUS_PATH = \"/home/guest/r12922050/GitHub/d2qplus/data/CSFCube-1.1/corpus.jsonl\"\n",
    "    DOC_TOPICS_PATH = \"/home/guest/r12922050/GitHub/d2qplus/augmented-data/CSFCube-1.1/topics/0609-pritamdeka_scibert-biobert-pos-keybert-mmr/doc_topics.jsonl\"\n",
    "\n",
    "    # Load your corpus\n",
    "    with open(CORPUS_PATH, \"r\") as f:\n",
    "        corpus = [json.loads(line) for line in f]\n",
    "    \n",
    "    with open(DOC_TOPICS_PATH, \"r\") as f:\n",
    "        doc_topics = [json.loads(line) for line in f]\n",
    "\n",
    "    print(f\"Processing {len(corpus)} documents with {len(doc_topics)} topic assignments\")\n",
    "    \n",
    "    # Initialize extractor\n",
    "    extractor = CorePhraseExtractor(\n",
    "        embedding_model=\"pritamdeka/S-Scibert-snli-multinli-stsb\",\n",
    "        device=\"cuda:1\"\n",
    "    )\n",
    "    \n",
    "    # Extract core phrases\n",
    "    core_phrases = extractor.extract_core_phrases(\n",
    "        corpus=corpus,\n",
    "        doc_topics=doc_topics,\n",
    "        top_n_candidates=50,      # More candidates for better selection\n",
    "        selection_ratio=0.25,     # Select top 25% most distinctive\n",
    "        min_phrases_per_doc=2,    # At least 2 phrases per document\n",
    "        max_phrases_per_doc=6,    # At most 6 phrases per document\n",
    "        keyphrase_ngram_range=(1, 3),\n",
    "        use_mmr=True,            # Use MMR for diversity\n",
    "        diversity=0.6            # Moderate diversity (0.0=no diversity, 1.0=max diversity)\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    output_path = \"/home/guest/r12922050/GitHub/d2qplus/augmented-data/CSFCube-1.1/keywords/core_phrases_ccqgen.jsonl\"\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for doc_id, phrases in core_phrases.items():\n",
    "            f.write(json.dumps({\"doc_id\": doc_id, \"core_phrases\": phrases}) + \"\\n\")\n",
    "    \n",
    "    print(f\"Core phrases saved to {output_path}\")\n",
    "    \n",
    "    # Show some examples\n",
    "    print(\"\\nExample results:\")\n",
    "    for i, (doc_id, phrases) in enumerate(list(core_phrases.items())[:3]):\n",
    "        print(f\"\\nDocument {doc_id}:\")\n",
    "        for phrase_info in phrases:\n",
    "            print(f\"  - '{phrase_info['phrase']}' (score: {phrase_info['distinctiveness_score']:.4f})\")\n",
    "    \n",
    "    return core_phrases\n",
    "\n",
    "# Run the extraction\n",
    "core_phrases_results = run_core_phrase_extraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9defed16",
   "metadata": {},
   "source": [
    "## Core Phrase Extraction using CCQGen Methodology\n",
    "\n",
    "This implementation extracts **distinctive core phrases** for each document based on the CCQGen paper methodology. The goal is to identify phrases that are:\n",
    "1. **Relevant** to the document (high semantic similarity)\n",
    "2. **Distinctive** within the document's topic group (not commonly used by similar documents)\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "**1. Topic-based Document Grouping**\n",
    "- Uses existing topic assignments (no need to re-run BERTopic)\n",
    "- Groups documents by shared topics, weighted by topic strength\n",
    "- Handles multi-topic documents through weighted averaging\n",
    "\n",
    "**2. BM25-based Distinctiveness Scoring**\n",
    "- For each candidate phrase, calculates BM25 relevance across topic group\n",
    "- Applies CCQGen formula: `exp(BM25_current) / (1 + sum(exp(BM25_others)))`\n",
    "- Higher scores = phrase is more distinctive to this document vs. topic peers\n",
    "\n",
    "**3. Enhanced KeyBERT Parameters**\n",
    "- `use_mmr=True` with `diversity=0.6` for non-redundant phrase selection\n",
    "- `nr_candidates=2x` gives MMR more options to choose diverse phrases\n",
    "- N-gram range (1,3) captures single words to 3-word phrases\n",
    "\n",
    "### Output Format:\n",
    "```json\n",
    "{\n",
    "  \"doc_id\": \"123\",\n",
    "  \"core_phrases\": [\n",
    "    {\"phrase\": \"neural architecture search\", \"distinctiveness_score\": 0.847},\n",
    "    {\"phrase\": \"automl\", \"distinctiveness_score\": 0.723}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Why this matters:** Standard keyword extraction might return \"deep learning\" for all ML papers, but this approach finds phrases like \"federated learning\" or \"graph neural networks\" that are specific to individual documents within the ML topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3d723d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 1177419\n",
      "Text: We present an architecture and an on-line learning algorithm and apply it to the problem of part-of-speech tagging. The architecture presented, SNOW, is a network of linear separators in the feature space, utilizing the Winnow update algorithm.Multiplicative weight-update algorithms such as Winnow have been shown to have exceptionally good behavior when applied to very high dimensional problems, and especially when the target concepts depend on only a small subset of the features in the feature space. In this paper we describe an architecture that utilizes this mistake-driven algorithm for multi-class prediction-selecting the part of speech of a word. The experimental analysis presented here provides more evidence to that these algorithms are suitable for natural language problems.The algorithm used is an on-line algorithm: every example is used by the algorithm only once, and is then discarded. This has significance in terms of efficiency, as well as quick adaptation to new contexts.We present an extensive experimental study of our algorithm under various conditions; in particular, it is shown that the algorithm performs comparably to the best known algorithms for POS.\n",
      "Core Phrases:\n",
      "  - 'architecture presented snow' (score: 0.4894)\n",
      "  - 'tagging architecture presented' (score: 0.4493)\n",
      "  - 'learning algorithm' (score: 0.4476)\n",
      "  - 'snow network linear' (score: 0.4209)\n",
      "  - 'algorithms suitable' (score: 0.4191)\n",
      "  - 'known algorithms pos' (score: 0.3874)\n",
      "\n",
      "\n",
      "Document ID: 2420674\n",
      "Text: Parallel texts (bitexts) have properties that distinguish them from other kinds of parallel data. First, most words translate to only one other word. Second, bitext correspondence is typically only partialmany words in each text have no clear equivalent in the other text. This article presents methods for biasing statistical translation models to reflect these properties. Evaluation with respect to independent human judgments has confirmed that translation models biased in this fashion are significantly more accurate than a baseline knowledge-free model. This article also shows how a statistical translation model can take advantage of preexisting knowledge that might be available about particular language pairs. Even the simplest kinds of language-specific knowledge, such as the distinction between content words and function words, are shown to reliably boost translation model performance on some tasks. Statistical models that reflect knowledge about the model domain combine the best of both the rationalist and empiricist paradigms.\n",
      "Core Phrases:\n",
      "  - 'distinguish kinds parallel' (score: 92.5002)\n",
      "  - 'bitext correspondence typically' (score: 40.9751)\n",
      "  - 'correspondence typically partialmany' (score: 40.9751)\n",
      "  - 'shown reliably boost' (score: 36.8001)\n",
      "  - 'clear equivalent text' (score: 35.4338)\n",
      "  - 'simplest kinds language' (score: 35.1979)\n",
      "\n",
      "\n",
      "Document ID: 10058082\n",
      "Text: We study the self-organization of the consonant inventories through a complex network approach. We observe that the distribution of occurrence as well as cooccurrence of the consonants across languages follow a power-law behavior. The co-occurrence network of consonants exhibits a high clustering coefficient. We propose four novel synthesis models for these networks (each of which is a refinement of the earlier) so as to successively match with higher accuracy (a) the above mentioned topological properties as well as (b) the linguistic property of feature economy exhibited by the consonant inventories. We conclude by arguing that a possible interpretation of this mechanism of network growth is the process of child language acquisition. Such models essentially increase our understanding of the structure of languages that is influenced by their evolutionary dynamics and this, in turn, can be extremely useful for building future NLP applications.\n",
      "Core Phrases:\n",
      "  - 'cooccurrence consonants languages' (score: 242.6805)\n",
      "  - 'economy exhibited consonant' (score: 110.8734)\n",
      "  - 'consonants exhibits high' (score: 110.7936)\n",
      "  - 'consonant inventories complex' (score: 78.1789)\n",
      "  - 'occurrence network consonants' (score: 77.4794)\n",
      "  - 'structure languages influenced' (score: 76.5033)\n",
      "\n",
      "\n",
      "Document ID: 2410895\n",
      "Text: BackgroundBioinformatics tools for automatic processing of biomedical literature are invaluable for both the design and interpretation of large-scale experiments. Many information extraction (IE) systems that incorporate natural language processing (NLP) techniques have thus been developed for use in the biomedical field. A key IE task in this field is the extraction of biomedical relations, such as protein-protein and gene-disease interactions. However, most biomedical relation extraction systems usually ignore adverbial and prepositional phrases and words identifying location, manner, timing, and condition, which are essential for describing biomedical relations. Semantic role labeling (SRL) is a natural language processing technique that identifies the semantic roles of these words or phrases in sentences and expresses them as predicate-argument structures. We construct a biomedical SRL system called BIOSMILE that uses a maximum entropy (ME) machine-learning model to extract biomedical relations. BIOSMILE is trained on BioProp, our semi-automatic, annotated biomedical proposition bank. Currently, we are focusing on 30 biomedical verbs that are frequently used or considered important for describing molecular events.ResultsTo evaluate the performance of BIOSMILE, we conducted two experiments to (1) compare the performance of SRL systems trained on newswire and biomedical corpora; and (2) examine the effects of using biomedical-specific features. The experimental results show that using BioProp improves the F-score of the SRL system by 21.45% over an SRL system that uses a newswire corpus. It is noteworthy that adding automatically generated template features improves the overall F-score by a further 0.52%. Specifically, ArgM-LOC, ArgM-MNR, and Arg2 achieve statistically significant performance improvements of 3.33%, 2.27%, and 1.44%, respectively.ConclusionWe demonstrate the necessity of using a biomedical proposition bank for training SRL systems in the biomedical domain. Besides the different characteristics of biomedical and newswire sentences, factors such as cross-domain framesets and verb usage variations also influence the performance of SRL systems. For argument classification, we find that NE (named entity) features indicating if the target node matches with NEs are not effective, since NEs may match with a node of the parsing tree that does not have semantic role labels in the training set. We therefore incorporate templates composed of specific words, NE types, and POS tags into the SRL system. As a result, the classification accuracy for adjunct arguments, which is especially important for biomedical SRL, is improved significantly.\n",
      "Core Phrases:\n",
      "  - 'biosmile uses maximum' (score: 58.6072)\n",
      "  - 'match node parsing' (score: 44.8193)\n",
      "  - 'incorporate templates composed' (score: 17.0622)\n",
      "  - '30 biomedical verbs' (score: 14.1012)\n",
      "  - 'prepositional phrases words' (score: 11.1328)\n",
      "  - 'especially important biomedical' (score: 9.7295)\n",
      "\n",
      "\n",
      "Document ID: 14688775\n",
      "Text: AbstractAutomatic sentiment classification has been extensively studied and applied in recent years. However, sentiment is expressed differently in different domains, and annotating corpora for every possible domain of interest is impractical. We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. First, we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline. Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains.\n",
      "Core Phrases:\n",
      "  - 'reducing relative error' (score: 8920.3796)\n",
      "  - 'investigate domain adaptation' (score: 5713.4630)\n",
      "  - 'potential adaptation classifier' (score: 1279.0918)\n",
      "  - 'domains annotating corpora' (score: 407.6700)\n",
      "  - 'domain adaptation sentiment' (score: 391.7397)\n",
      "  - 'classification extensively studied' (score: 160.6423)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/guest/r12922050/GitHub/d2qplus/augmented-data/CSFCube-1.1/keywords/core_phrases_ccqgen.jsonl\", \"r\") as f:\n",
    "    core_phrases = [json.loads(line) for line in f]\n",
    "\n",
    "CORPUS_PATH = \"/home/guest/r12922050/GitHub/d2qplus/data/CSFCube-1.1/corpus.jsonl\"\n",
    "with open(CORPUS_PATH, \"r\") as f:\n",
    "    corpus = [json.loads(line) for line in f]\n",
    "\n",
    "docid2text = {doc[\"_id\"]: doc[\"text\"] for doc in corpus}\n",
    "\n",
    "# random pick 5 objects in core_phrases\n",
    "import random\n",
    "random.seed(42)  # For reproducibility\n",
    "sampled_core_phrases = random.sample(core_phrases, 5)\n",
    "for item in sampled_core_phrases:\n",
    "    doc_id = item[\"doc_id\"]\n",
    "    phrases = item[\"core_phrases\"]\n",
    "    print(f\"Document ID: {doc_id}\")\n",
    "    print(f\"Text: {docid2text[doc_id]}\")  # Print first 200 chars of text\n",
    "    print(\"Core Phrases:\")\n",
    "    for phrase_info in phrases:\n",
    "        print(f\"  - '{phrase_info['phrase']}' (score: {phrase_info['distinctiveness_score']:.4f})\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c78a85a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved updated corpus with predicted queries to /home/guest/r12922050/GitHub/d2qplus/gen/CSFCube-1.1/text_add_phrase_mining_keywords.jsonl\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/guest/r12922050/GitHub/d2qplus/augmented-data/CSFCube-1.1/keywords/core_phrases_ccqgen.jsonl\", \"r\") as f:\n",
    "    core_phrases = [json.loads(line) for line in f]\n",
    "\n",
    "CORPUS_PATH = \"/home/guest/r12922050/GitHub/d2qplus/data/CSFCube-1.1/corpus.jsonl\"\n",
    "with open(CORPUS_PATH, \"r\") as f:\n",
    "    corpus = [json.loads(line) for line in f]\n",
    "\n",
    "docid2phrases = {item[\"doc_id\"]: item[\"core_phrases\"] for item in core_phrases}\n",
    "\n",
    "for doc in corpus:\n",
    "    doc_id = doc.pop(\"_id\")\n",
    "    doc['id'] = doc_id\n",
    "    phrases = docid2phrases.get(doc_id, [])\n",
    "    predicted_queries = []\n",
    "    for phrase_info in phrases:\n",
    "        predicted_queries.append(phrase_info['phrase'])\n",
    "    doc[\"predicted_queries\"] = predicted_queries\n",
    "# Save the updated corpus with predicted queries\n",
    "OUTPUT_PATH = \"/home/guest/r12922050/GitHub/d2qplus/gen/CSFCube-1.1/text_add_phrase_mining_keywords.jsonl\"\n",
    "with open(OUTPUT_PATH, \"w\") as f:\n",
    "    for doc in corpus:\n",
    "        f.write(json.dumps(doc) + \"\\n\")\n",
    "print(f\"Saved updated corpus with predicted queries to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aa83e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'phrase': 'endpoint projection type',\n",
       "  'distinctiveness_score': 47828.84120350538},\n",
       " {'phrase': 'parameterised mpi programs',\n",
       "  'distinctiveness_score': 44512.655180433576},\n",
       " {'phrase': 'dimensions parameterised protocols',\n",
       "  'distinctiveness_score': 26746.3934717538},\n",
       " {'phrase': 'pabble guarantee safety',\n",
       "  'distinctiveness_score': 18875.86860069233},\n",
       " {'phrase': 'protocols type checking',\n",
       "  'distinctiveness_score': 9923.528223115984},\n",
       " {'phrase': 'local protocols type',\n",
       "  'distinctiveness_score': 7771.446169015394}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_phrases[0]['core_phrases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136568c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2qplus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
